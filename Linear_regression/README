#  Linear Regression

###  What Is It
Linear Regression models the relationship between one or more input features and a continuous target variable by fitting a straight line (or hyperplane) through the data.

\[
\hat{y} = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b
\]

---

###  When Do We Use It
- When the output variable is **continuous** (e.g., price, sales, temperature).  
- When the relationship between inputs and outputs appears **linear**.  
- When model **interpretability** is important.  

---

###  Why Do We Use It
- To understand how features influence outcomes.  
- To establish a **baseline model** for regression tasks.  
- To get fast, interpretable predictions.

---

###  Main Reasons to Use It
1. **Simplicity:** Quick to train and easy to explain.  
2. **Interpretability:** Coefficients show direct influence of features.  
3. **Efficiency:** Low computational cost.  
4. **Baseline Power:** Ideal first model before complex algorithms.  

---

###  Key Functions

| Function | Purpose | Formula |
|-----------|----------|----------|
| **Prediction** | Estimate output | \( \hat{y} = w x + b \) |
| **Cost Function (MSE)** | Measures prediction error | \( J(w,b) = \frac{1}{2m}\sum (\hat{y}-y)^2 \) |
| **Gradient Descent** | Optimize weights | \( w := w - \alpha \frac{\partial J}{\partial w} \) |

---

© 2025 — Machine Learning Practice Repository  
Author: *Your Name*
